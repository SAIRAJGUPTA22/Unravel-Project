# Unravel Project

This project is a Python-based news aggregation pipeline that collects, parses, and stores the latest articles from two leading travel industry news sources: **Skift** and **PhocusWire**. Its primary goal is to maintain an up-to-date, deduplicated repository of travel news for streamlined access and analysis.

## 1. Data Sources

- **Skift:** Provides structured metadata, including reliable published dates in ISO format.
- **PhocusWire:** Lacks consistent published dates on homepage articles, necessitating fallback handling.

## 2. Extraction Process

The scraper retrieves the homepages of both sources and extracts the following details for each article:

- **Title**
- **URL**
- **Summary** (when available)
- **Published Date**
- **Unique Article ID**: Generated as an MD5 hash of the URL and title.

**Source-Specific Handling:**

- **Skift:** Published dates are extracted from `<time>` tags in ISO format.
- **PhocusWire:** Due to the absence of published dates on the homepage, the current UTC time is used as a fallback.

## 3. Database Management

- Utilizes an **SQLite database** to store article details and published timestamps.
- Ensures **no duplicate entries** by checking unique article IDs before insertion.
- Maintains synchronization metadata to support incremental updates.

## 4. Incremental Updates

- On subsequent runs, the pipeline processes **only new articles** by comparing their published dates against the last synchronization time.
- **Skift:** Supports effective incremental syncing due to reliable published dates.
- **PhocusWire:** All homepage articles are treated as new on each run because of unreliable published dates, leading to repeated processing.

## 5. Handling PhocusWire Limitations

- The lack of real published dates on PhocusWire's homepage prevents reliable incremental filtering.
- As a result, every homepage article from PhocusWire is reprocessed during each run.
- Future enhancements may include scraping individual article pages to extract accurate published dates or leveraging alternative metadata if available.

## 6. Pipeline Execution and Logging

**Execution Steps:**

- Fetch articles from both Skift and PhocusWire.
- Filter out duplicates within the same run.
- Perform incremental synchronization based on published dates for Skift articles.

**Logging:**

- Duplicate detections are logged **exclusively** in `logs/duplicates.log` to maintain a clean main console.
- The main log file (`news_pipeline.log`) records all pipeline activities and errors.
- Console output displays:
  - Number of articles fetched per source.
  - Number of new articles inserted.
  - Details of the latest 5 articles.

## 7. Scheduling
- The pipeline can be executed manually or scheduled to run hourly.
- Manual Run:
    python main.py
- Scheduled Run (Every Hour):
    python main.py --schedule
- When scheduled, the script remains active, executing scraping jobs automatically to ensure the news database stays current throughout the day.

When scheduled, the script remains active, executing scraping jobs automatically to ensure the news database stays current throughout the day.

## Duplicate Checks — How They Are Performed

### In-Memory Duplicate Detection (Within a Single Run):

- The pipeline uses an **in-memory set** (usually named `all_ids` or `seen_ids`) to keep track of every article ID processed in the **current execution**.
- Each article has a **unique ID generated by hashing its URL and title**.
- If an article ID already exists in this set during the run, the article is considered a duplicate **within the same run** and is **skipped**, preventing repeated inserts of the same article arriving multiple times.
- These duplicate detections are logged to a **separate file (`logs/duplicates.log`)** to avoid cluttering the console and main logs.

### Database-Level Duplicate Prevention:

- Before inserting an article, the pipeline checks the **last synchronization time** of that source.
- Articles that are **older or equal to the last sync timestamp** are **skipped** because they were already processed in a previous run.
- The insertion function ensures that articles with the same unique ID do not create multiple records in the database.

## Handling Article Updates (Optional Enhancement)

Currently, the pipeline **does not update existing articles** — it only inserts new ones and skips duplicates. However, if we want to handle **updates to articles** (for example, if titles, summaries, or other metadata change over time), we can follow this suggested approach:

### Proposed Process to Handle Article Updates

 **Add a New Column to  Articles Table**

   - Create a column, e.g., called `content_hash` or `checksum`, which stores a hash or checksum representing the current state of the article's content (title, summary, author, or any relevant fields).

 **During Each Pipeline Run:**

   - For each article fetched, compute the hash/checksum of the *important* fields we want to track for changes (e.g., title + summary + published date).
   - Query the database to check if the article already exists:
     - If the article **does not exist**, insert it normally.
     - If the article **exists**, compare the new checksum with the stored one.
       - If the checksums are **equal**, it means nothing changed — skip updating.
       - If the checksums differ, update the article in the database with the new values and update the checksum column.



