
The  Project is a Python-based news aggregation pipeline that collects, parses, and stores the latest articles from two leading travel industry news sources: Skift and PhocusWire. Its primary goal is to maintain an up-to-date, deduplicated repository of travel news for streamlined access and analysis.

1. Data Sources
- Skift: Provides structured metadata, including reliable published dates in ISO format.
- PhocusWire: Lacks consistent published dates on homepage articles, necessitating fallback handling.

2. Extraction Process
The scraper retrieves the homepages of both sources and extracts the following details for each article:
  • Title
  • URL
  • Summary (when available)
  • Published Date
  • Unique Article ID: Generated as an MD5 hash of the URL and title.

Source‑Specific Handling:
- Skift: Published dates are extracted from <time> tags in ISO format.
- PhocusWire: Due to the absence of published dates on the homepage, the current UTC time is used as a fallback.

3. Database Management
- Utilizes an SQLite database to store article details and published timestamps.
- Ensures no duplicate entries by checking unique article IDs before insertion.
- Maintains synchronization metadata to support incremental updates.

4. Incremental Updates
- On subsequent runs, the pipeline processes only new articles by comparing their published dates against the last synchronization time.
- Skift: Supports effective incremental syncing due to reliable published dates.
- PhocusWire: All homepage articles are treated as new on each run because of unreliable published dates, leading to repeated processing.

5. Handling PhocusWire Limitations
- The lack of real published dates on PhocusWire's homepage prevents reliable incremental filtering.
- As a result, every homepage article from PhocusWire is reprocessed during each run.
- Future enhancements may include scraping individual article pages to extract accurate published dates or leveraging alternative metadata if available.

6. Pipeline Execution and Logging
Execution Steps:
  • Fetch articles from both Skift and PhocusWire.
  • Filter out duplicates within the same run.
  • Perform incremental synchronization based on published dates for Skift articles.

Logging:
  • Duplicate detections are logged exclusively in logs/duplicates.log to maintain a clean main console.
  • The main log file (news_pipeline.log) records all pipeline activities and errors.
  • Console output displays:
    - Number of articles fetched per source.
    - Number of new articles inserted.
    - Details of the latest 5 articles.

7. Scheduling
- The pipeline can be executed manually or scheduled to run hourly.
- Manual Run:
    python main.py
- Scheduled Run (Every Hour):
    python main.py --schedule
- When scheduled, the script remains active, executing scraping jobs automatically to ensure the news database stays current throughout the day.



